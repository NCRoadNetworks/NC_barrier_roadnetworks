{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ede87f",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Identify the elevation and exceedance probability of the critical node that causes the network's failure and the overall robustness of each road network to flood-induced failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582e448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contextily as ctx\n",
    "import statistics\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d57fc220",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set working directory\n",
    "\n",
    "path='..' # introduce path to your working directory\n",
    "# os.chdir(path) # In this notebook, this command cannot be used because it triggers a JSONDecodeError when GA9 is downloaded\n",
    "# To avoid the error and be able to download all road networks, the path to the working directory needs to be set as an absolute path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308b9d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downeast2\n",
      "NC14\n"
     ]
    },
    {
     "ename": "DriverError",
     "evalue": "../data/Downeast/NC14.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/_shim.pyx:83\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ../data/Downeast/NC14.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 129\u001b[0m\n\u001b[1;32m    124\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m### create maps for each network using OSM as basemap \u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# read polygons\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m poly \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{0}\u001b[39;49;00m\u001b[38;5;124;43m/data/Downeast/\u001b[39;49m\u001b[38;5;132;43;01m{1}\u001b[39;49;00m\u001b[38;5;124;43m.shp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbarrier\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# extract just the geometry (shapely object) part and clean it with a buffer\u001b[39;00m\n\u001b[1;32m    131\u001b[0m poly_geo \u001b[38;5;241m=\u001b[39m poly[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/geopandas/io/file.py:253\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_file_pyogrio(\n\u001b[1;32m    258\u001b[0m         path_or_bytes, bbox\u001b[38;5;241m=\u001b[39mbbox, mask\u001b[38;5;241m=\u001b[39mmask, rows\u001b[38;5;241m=\u001b[39mrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    259\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/geopandas/io/file.py:294\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[1;32m    295\u001b[0m \n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m# In a future Fiona release the crs attribute of features will\u001b[39;00m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# no longer be a dict, but will behave like a dict. So this should\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;66;03m# be forwards compatible\u001b[39;00m\n\u001b[1;32m    299\u001b[0m         crs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    300\u001b[0m             features\u001b[38;5;241m.\u001b[39mcrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    301\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[1;32m    303\u001b[0m         )\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;66;03m# handle loading the bounding box\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/fiona/env.py:408\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local\u001b[38;5;241m.\u001b[39m_env:\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/fiona/__init__.py:264\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     path \u001b[38;5;241m=\u001b[39m parse_path(fp)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 264\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m schema:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;66;03m# Make an ordered dict of schema properties.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/fiona/collection.py:162\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[0;32mfiona/ogrext.pyx:540\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_shim.pyx:90\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: ../data/Downeast/NC14.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "### Explore the size decay of the GCC to identify the critical node that leads to the fragmentation of the network (road networks with more than 100 nodes) and plot maps with road networks\n",
    "\n",
    "# Create folders if they don't exist\n",
    "outdir= '{0}/Results'.format(path)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "outdir= '{0}/Results/GCC_Plots'.format(path)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "outdir= '{0}/Results/Networks_Maps'.format(path)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "\n",
    "# Loop through files to open each barrier graphml\n",
    "rootdir = '{0}/data/Roads'.format(path)\n",
    "extensions = ('.graphml')\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            barrier = file.replace(\".graphml\",\"\")\n",
    "            print(barrier)\n",
    "            G = nx.read_graphml(file_path)\n",
    "            N = len(G.nodes(data=True))\n",
    "            GCCs=[] # list with the number of connected components and its size\n",
    "            if N>100:\n",
    "                # pull out elevation attribute\n",
    "                Z = nx.get_node_attributes(G,'Elevations')\n",
    "                # convert str values in float to be able to sort them \n",
    "                Z = dict(zip(Z.keys(), [float(value) for value in Z.values()]))\n",
    "                # sort elevation values in ascending order\n",
    "                Sorted_Z = sorted(Z.items(), key=lambda item: item[1])\n",
    "                # select first element of each tuple in the list (nodes ID):\n",
    "                FT = [i[0] for i in Sorted_Z]\n",
    "                # Select second element of each tuple in the list (elevation) and convert to float\n",
    "                ST = [i[1] for i in Sorted_Z]\n",
    "                for i in range(len(ST)):\n",
    "                    ST[i] = float(ST[i])\n",
    "                # create array \n",
    "                CCs = np.zeros([len(Sorted_Z),2])\n",
    "                # loop through all nodes\n",
    "                for i in range(0, len(FT)):\n",
    "                    # find the node with lowest elevation from the list using i and remove it\n",
    "                    G.remove_nodes_from(FT[0:i])\n",
    "                    # find the number of connected components and its respective size\n",
    "                    GCC = [len(c)\n",
    "                            for c in sorted(nx.weakly_connected_components(G), key=len, reverse=True)]\n",
    "                    GCCs.append(GCC) \n",
    "                    # fill array, first column corresponds to FGC (first giant component), second column to SGC (second giant component)\n",
    "                    if len(GCC)==1:\n",
    "                        CCs[int(i),0]=GCC[0]/len(FT)\n",
    "                        CCs[int(i),1]=0\n",
    "                    else:\n",
    "                        CCs[int(i),0]=GCC[0]/len(FT)\n",
    "                        CCs[int(i),1]=GCC[1]/len(FT)\n",
    "                # find the node that, once removed, the size of the FGC drops abruptly while the size of the SGC reaches its maximum\n",
    "                m = max(CCs[:,1])\n",
    "                pos=[i for i, j in enumerate(CCs[:,1]) if j == m]\n",
    "                pos= pos[0] # position of max value in SGC\n",
    "                critical= pos-1 # position of the critical node whose removal causes the percolation transition.\n",
    "                elev=ST[critical] # find elevation of the critical node\n",
    "                removed=pos # number of nodes removed when percolation threshold occurs\n",
    "\n",
    "                # plot\n",
    "                col1=[] \n",
    "                for i in range(0,len(FT)):\n",
    "                    if i==critical:\n",
    "                        col1.append('#D53032') \n",
    "                    else:\n",
    "                        col1.append('#000000')  \n",
    "                col2=[]\n",
    "                for i in range(0,len(FT)):\n",
    "                    if i==critical:\n",
    "                        col2.append('#D53032') \n",
    "                    else:\n",
    "                        col2.append('#808080') \n",
    "                col3=[]\n",
    "                for i in range(0,len(FT)):\n",
    "                    if i==critical:\n",
    "                        col3.append('#D53032') \n",
    "                    else:\n",
    "                        col3.append('#9ACD32') \n",
    "\n",
    "                f, (ax1,ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "                x_coord = 1 * np.arange(len(FT))/len(FT) # fraction of nodes removed\n",
    "                ax1.plot(x_coord, CCs[:,0],':ok') # FGC\n",
    "                for i in range (len(FT)):\n",
    "                    ax1.plot(x_coord[i],CCs[i,0],'o', markersize=5, color=col1[i]) # plot with two colors to highlight critical node\n",
    "                ax1.set_ylabel(\"First Giant Component Size\")\n",
    "                ax3 = ax1.twinx()\n",
    "                ax3.plot(x_coord, CCs[:,1],':ok') # SGC\n",
    "                for i in range (len(FT)):\n",
    "                    ax3.plot(x_coord[i],CCs[i,1],'o', markersize=5, color=col2[i]) \n",
    "                ax3.set_ylabel(\"Second Giant Component Size\")\n",
    "                ax2.plot(x_coord,CCs[:,0],':ok') # FGC\n",
    "                for i in range (len(FT)):\n",
    "                    ax2.plot(x_coord[i],CCs[i,0],'o', markersize=5, color=col1[i]) \n",
    "                ax2.set_ylabel(\"First Giant Component Size\")\n",
    "                ax4 = ax2.twinx()\n",
    "                ax4.plot(x_coord,ST,':o', color='#9ACD32') # elevation\n",
    "                for i in range (len(FT)):\n",
    "                    ax4.plot(x_coord[i],ST[i],'o', markersize=5, color=col3[i]) \n",
    "                ax2.set_ylabel(\"First Giant Component Size\",)\n",
    "                ax4.set_ylabel(\"Elevation\")\n",
    "                ax2.set_xlabel(\"Fraction of removed nodes\") \n",
    "                legend_elements1 = [Line2D([0], [0], marker='o', color='#000000', label='FGC', markersize=10),\n",
    "                                    Line2D([0], [0], marker='o', color='#808080', label='SGC', markersize=10),\n",
    "                                    Line2D([0], [0], marker='o', color='#D53032', label='Critical node', markersize=10)]\n",
    "                ax1.legend(handles=legend_elements1, loc=\"best\", frameon=False, fontsize=18)\n",
    "                legend_elements2 = [Line2D([0], [0], marker ='o', color='#000000', label='FGC', markersize=10),\n",
    "                                   Line2D([0], [0], marker='o', color='#9ACD32', label='Elevation', markersize=10)]\n",
    "                ax1.legend(handles=legend_elements1, loc=\"best\", frameon=False, fontsize=18)\n",
    "\n",
    "                plt.rcParams[\"font.size\"]= 20\n",
    "                plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "\n",
    "                f.savefig(\"{0}/Results/GCC_Plots/{1}.png\".format(path,barrier), dpi=500, facecolor='w')\n",
    "                plt.close(\"all\")\n",
    "\n",
    "                ### create maps for each network using OSM as basemap \n",
    "\n",
    "                # read polygons\n",
    "                poly = gpd.read_file(\"{0}/data/Downeast/{1}.shp\".format(path,barrier))\n",
    "                # extract just the geometry (shapely object) part and clean it with a buffer\n",
    "                poly_geo = poly['geometry'].iloc[0]\n",
    "                poly_geo = poly_geo.buffer(0)\n",
    "                poly_geo.is_valid\n",
    "                # extract drivable network and project it\n",
    "                graph = ox.graph_from_polygon(poly_geo, network_type='drive', simplify=True, clean_periphery=True)\n",
    "                # retrieve nodes and edges as geodataframes\n",
    "                nodes, edges = ox.graph_to_gdfs(graph)\n",
    "                # create an index for the geodataframe nodes\n",
    "                nodes['index'] = range(0, len(nodes))\n",
    "\n",
    "                # convert Z dict in pandas dataframe and name columns\n",
    "                Z = pd.DataFrame(list(Z.items()),columns = ['index','elevation'])\n",
    "                # convert all columns in numerics so there are no errors when merging\n",
    "                Z = Z.apply(pd.to_numeric)\n",
    "                # join pandas dataframe to nodes geodataframe using 'index' so that the gdf has elevation\n",
    "                nodes = nodes.merge(Z, on='index')\n",
    "\n",
    "                # create new columns for color and size\n",
    "                def color(row):\n",
    "                    if row['elevation'] < elev:\n",
    "                        val = \"black\"\n",
    "                    elif row['elevation']== elev:\n",
    "                        val = \"red\"\n",
    "                    else:\n",
    "                        val = \"green\"\n",
    "                    return val\n",
    "\n",
    "                def size(row):\n",
    "                    if row['elevation'] == elev:\n",
    "                        val = 50\n",
    "                    else:\n",
    "                        val = 30\n",
    "                    return val\n",
    "\n",
    "                nodes['Color'] = nodes.apply(color, axis=1) # new column with color categories \n",
    "                nodes['Size'] = nodes.apply(size, axis=1) # new column with size categories\n",
    "\n",
    "                # plot map\n",
    "                fig, ax = plt.subplots()\n",
    "                nodes = nodes.to_crs(epsg=3857) # convert gdf to EPSG used by basemaps\n",
    "                edges = edges.to_crs(epsg=3857)\n",
    "                nodes.plot(ax=ax, color=nodes.Color, markersize=nodes.Size, zorder=2, legend=True) # plot nodes\n",
    "                edges.plot(ax=ax, alpha=0.2, color='black', zorder=1) # plot edges\n",
    "                ctx.add_basemap(ax, zoom=13, source=ctx.providers.OpenStreetMap.Mapnik) # add basemap (OSM)\n",
    "                plt.xticks(fontsize=12) # reduce fontsize of x axis\n",
    "                plt.yticks(fontsize=12) # reduce fontsize of y axis\n",
    "                legend_elements = [Line2D([0], [0], marker='o', color='black', label='Connected nodes',\n",
    "                                          markerfacecolor='g', markersize=10),\n",
    "                                   Line2D([0], [0], marker='o', color='black', label='Disconnected nodes',\n",
    "                                          markerfacecolor='b', markersize=10),\n",
    "                                   Line2D([0], [0], marker='o', color='black', label='Target node',\n",
    "                                          markerfacecolor='r', markersize=10),\n",
    "                                   ] # create legend\n",
    "                ax.legend(handles=legend_elements, loc='best', frameon=False)\n",
    "                ax.set_title(barrier, fontsize=22)\n",
    "                ax.ticklabel_format(style='plain')         \n",
    "                plt.rcParams[\"figure.figsize\"] = (25,25)\n",
    "                plt.savefig('{0}/Results/Networks_Maps/{1}.png'.format(path,barrier), dpi=300, facecolor='w')   \n",
    "                plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c2a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create table with results for all barriers with drivable networks \n",
    "\n",
    "barriers=[] # barrier name\n",
    "n_nodes=[] # number of nodes\n",
    "r=[] # robustness \n",
    "min_z=[] # min node elevation in the network\n",
    "max_z=[] # max node elevation in the network\n",
    "mean_z=[] # mean node elevation\n",
    "median_z=[] # median node elevation\n",
    "critical_z=[] # elevation critical node\n",
    "critical_e=[] # exceedance probability critical node (given in return period)\n",
    "removed_nodes=[] # number of nodes removed when critical node is removed\n",
    "removed_perc=[] # percentage of nodes removed when critical node is removed\n",
    "threshold=[] # value critical threshold\n",
    "\n",
    "\n",
    "rootdir = '{0}/data/Roads'.format(path)\n",
    "extensions = ('.graphml')\n",
    "\n",
    "# Loop through files and open barrier graphml\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            barrier = file.replace(\".graphml\",\"\")\n",
    "            G = nx.read_graphml(file_path)\n",
    "            N= len(G.nodes(data=True))\n",
    "            if N>100:\n",
    "                GCCs=[]\n",
    "                barriers.append(barrier) \n",
    "                n_nodes.append(N) \n",
    "                # pull out elevation attribute\n",
    "                Z = nx.get_node_attributes(G,'Elevations')\n",
    "                # convert str values in float to be able to sort them \n",
    "                Z = dict(zip(Z.keys(), [float(value) for value in Z.values()]))\n",
    "                # sort it based on elevation, min first\n",
    "                Sorted_Z = sorted(Z.items(), key=lambda item: item[1])\n",
    "                CCs = np.zeros([len(Sorted_Z),2])\n",
    "                # select first element of each tuple in the list (nodes ID):\n",
    "                FT = [i[0] for i in Sorted_Z]\n",
    "                # select second element of each tuple in the list (elevation) and convert to float\n",
    "                ST = [i[1] for i in Sorted_Z]\n",
    "                for i in range(len(ST)):\n",
    "                    ST[i] = float(ST[i])\n",
    "\n",
    "                # calculate elevation stats \n",
    "                min_elev=min(ST)\n",
    "                min_z.append(min_elev)\n",
    "                max_elev=max(ST)\n",
    "                max_z.append(max_elev)\n",
    "                mean_elev = statistics.mean(ST)\n",
    "                mean_z.append(mean_elev)\n",
    "                median_elev = statistics.median(ST)\n",
    "                median_z.append(median_elev)\n",
    "                \n",
    "                # remove nodes by elevation and calculate size of first and second components\n",
    "                for i in range(0, len(FT)):\n",
    "                    # find the node with lowest elevation from the list using i and remove it\n",
    "                    G.remove_nodes_from(FT[0:i])\n",
    "                    # find the number of connected components and its respective size\n",
    "                    GCC = [len(c)\n",
    "                            for c in sorted(nx.weakly_connected_components(G), key=len, reverse=True)]\n",
    "                    GCCs.append(GCC) # list with the number of connected components and its size\n",
    "                    # fill array, first column corresponds to FGC (first giant component), second column to SGC (second giant component)\n",
    "                    if len(GCC)==1:\n",
    "                        CCs[int(i),0]=GCC[0]/len(FT)\n",
    "                        CCs[int(i),1]=0\n",
    "                    else:\n",
    "                        CCs[int(i),0]=GCC[0]/len(FT)\n",
    "                        CCs[int(i),1]=GCC[1]/len(FT)\n",
    "\n",
    "                # find the node that, once removed, the FGC decreases and the SGC reaches its maximum (critical threshold)\n",
    "                m = max(CCs[:,1])\n",
    "                pos=[i for i, j in enumerate(CCs[:,1]) if j == m]\n",
    "                pos= pos[0] # position of max value in SGC\n",
    "                critical= pos-1 # position of the critical node whose removal causes the percolation transition.\n",
    "                elev=ST[critical] # find elevation of the critical node\n",
    "                critical_z.append(elev)\n",
    "                removed=pos # number of nodes removed when percolation threshold occurs\n",
    "                removed_nodes.append(removed)\n",
    "                perc_removed=int(removed)/N*100\n",
    "                removed_perc.append(perc_removed)\n",
    "                x_coord = 1 * np.arange(len(FT))/len(FT) # Fraction of nodes removed\n",
    "                thresh= x_coord[critical]\n",
    "                threshold.append(thresh)\n",
    "\n",
    "                # exceedance probability for the critical node\n",
    "                exceed = pd.read_csv(\"{0}/data/Exceedance/Probability/{1}_Exceedance.csv\".format(path,barrier), sep=\",\", header=0)\n",
    "                exceed_x= exceed.MaxWL\n",
    "                exceed_y= exceed.Probability\n",
    "                node_elev= elev\n",
    "                exceedance= np.interp(node_elev, exceed_x, exceed_y)\n",
    "                critical_e.append(exceedance)\n",
    "\n",
    "                # calculate robustness following Schneider's equation (2011) \n",
    "                s= sum(CCs[:,0])\n",
    "                rob= s/len(FT)\n",
    "                r.append(rob)\n",
    "            else:\n",
    "                continue\n",
    "table = list(zip(barriers,n_nodes,r,min_z,max_z,mean_z,median_z,critical_z,critical_e,removed_nodes,removed_perc,threshold))\n",
    "table = pd.DataFrame(table, columns=['Barrier','Nodes','Robustness','Min_elevation','Max_elevation','Mean_elevation','Median_elevation','Critical_elevation','Critical_exceedance','Removed_nodes','Removed_%','Critical_threshold'])\n",
    "table.to_csv('{0}/Results/Results_Downeast.csv'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50516cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downeast2\n"
     ]
    }
   ],
   "source": [
    "### For each network, calculate basic statistics using OSMnx package\n",
    "\n",
    "# Create folders if it doesn't exist\n",
    "outdir= '{0}/Results/Statistics'.format(path)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "rootdir = \"{0}/data/Downeast\".format(path)\n",
    "extensions = ('.shp')\n",
    "\n",
    "table = pd.read_csv(\"{0}/Results/Results_Downeast.csv\".format(path), sep=\",\", header=0)\n",
    "developed_barriers= list(table.Barrier) # to calculate statistics only for the selected barrier islands (those with more than 72 nodes)\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            barrier = file.replace(\".shp\",\"\")\n",
    "            barrier = barrier.replace(\"_geo\",\"\")\n",
    "            print(barrier)\n",
    "            if barrier in developed_barriers:\n",
    "                # read polygons\n",
    "                poly = gpd.read_file(file_path)\n",
    "                # extract just the geometry (shapely object) part and clean it with a buffer\n",
    "                poly_geo = poly['geometry'].iloc[0]\n",
    "                poly_geo = poly_geo.buffer(0)\n",
    "                poly_geo.is_valid\n",
    "                # project polygon to calculate area\n",
    "                poly_prj=ox.project_gdf(poly)\n",
    "                area=float(poly_prj.area)\n",
    "                # pull network\n",
    "                G = ox.graph_from_polygon(poly_geo, network_type='drive', simplify=True, clean_periphery=True)\n",
    "                if len(G.nodes(data=True))>100:\n",
    "                    # project it and calculate statistics\n",
    "                    G_proj = ox.project_graph(G)\n",
    "                    stats = ox.basic_stats(G_proj, area=area) #, circuity_dist='euclidean')\n",
    "\n",
    "                    # delete the no longer needed dict elements\n",
    "                    del stats['streets_per_node_counts']\n",
    "                    del stats['streets_per_node_proportions']\n",
    "\n",
    "                    # load as a pandas dataframe\n",
    "                    df = pd.DataFrame.from_dict(stats, orient='index')\n",
    "                    df.columns= [barrier]\n",
    "                    df.to_csv('{0}/Results/Statistics/{1}.csv'.format(path,barrier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37813a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
