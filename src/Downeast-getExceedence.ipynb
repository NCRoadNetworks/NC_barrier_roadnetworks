{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd42a9b",
   "metadata": {},
   "source": [
    "##  Calculate exceedance probabilities of extreme water levels for NC barrier islands -\n",
    "\n",
    "The purpose of this notebook is to calculate exceedance probabilities of extreme water levels for Downeast using a GEV approach. The parameters (scale, shape, and location) for the closest NOAA CO-OPS (Center for Operational Oceanographic Products and Services) station to each barrier island were extracted from the NOAA Technical Report NOS CO-OPS 067 (Zervas, 2013) available in https://tidesandcurrents.noaa.gov/publications/NOAA_Technical_Report_NOS_COOPS_067a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0527f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import genextreme\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdbaaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set working directory\n",
    "\n",
    "path='..' # introduce path to your working directory\n",
    "os.chdir(path)\n",
    "\n",
    "### Merge all barrier polygons in one single shp \n",
    "\n",
    "# Create folder if it does no exist\n",
    "outdir= './data/Exceedance'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "# Read all shp and merge them in one single dataset\n",
    "folder = Path(\"./data/Downeast\")\n",
    "shapefiles = folder.glob(\"*.shp\")\n",
    "gdf = pd.concat([\n",
    "    gpd.read_file(shp)\n",
    "    for shp in shapefiles\n",
    "]).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# For some barrier islands, the column \"name\" has a different format. Convert name to same format so they can be matched later\n",
    "# gdf.loc[gdf.name == 'NC1   -           Core        Banks', 'name'] = 'NC1'\n",
    "# gdf.loc[gdf.name == 'NC2-        Cape     Lookout, NC', 'name'] = 'NC2'\n",
    "# gdf.loc[gdf.name == 'NC3  ShacklefordBanks,     NC', 'name'] = 'NC3'\n",
    "# gdf.loc[gdf.name == 'NC4-        Bogue    Banks,   NC', 'name'] = 'NC4'\n",
    "# gdf.loc[gdf.name == 'NC12Ocracoke  Island,   NC', 'name'] = 'NC12'\n",
    "\n",
    "\n",
    "gdf.to_file('./data/Exceedance/Downeast.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6580325e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((-76.75868 34.92539, -76.65840 34.994...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           geometry\n",
       "0  None  POLYGON ((-76.75868 34.92539, -76.65840 34.994..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barriers= gpd.read_file('./data/Exceedance/Downeast.shp')\n",
    "barriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70013c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebgoldstein/anaconda3/envs/roads/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:122: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n",
      "/tmp/ipykernel_2019286/3457588174.py:7: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  gdf.to_file(\"./data/Exceedance/Stations.shp\")\n"
     ]
    }
   ],
   "source": [
    "### Read stations.csv and convert to geodataframe \n",
    "\n",
    "df = pd.read_csv(\"./data/tables/Stations.csv\", sep=\",\", header=0) \n",
    "df[\"geometry\"] = df[[\"Longitude\", \"Latitude\"]].apply(Point, axis=1)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "gdf.to_file(\"./data/Exceedance/Stations.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b971ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m station_number\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(barriers)):\n\u001b[0;32m---> 14\u001b[0m     name\u001b[38;5;241m=\u001b[39m \u001b[43mbarriers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i]\n\u001b[1;32m     15\u001b[0m     barrier_name\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m     16\u001b[0m     barrier\u001b[38;5;241m=\u001b[39m barriers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/geopandas/geodataframe.py:1428\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;124;03m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;124;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;124;03m    return a GeoDataFrame.\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1428\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     geo_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_geometry_column_name\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mdtype, GeometryDtype):\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/pandas/core/indexes/base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'"
     ]
    }
   ],
   "source": [
    "### Find nearest station to each barrier and its corresponding distance \n",
    "\n",
    "barriers= gpd.read_file('./data/Exceedance/Downeast.shp')\n",
    "stations= gpd.read_file('./data/Exceedance/Stations.shp')\n",
    "\n",
    "barriers= barriers.to_crs('esri:102009')\n",
    "stations= stations.to_crs('esri:102009')\n",
    "\n",
    "barrier_name=[]\n",
    "min_distance=[]\n",
    "station_number=[]\n",
    "\n",
    "for i in range(0,len(barriers)):\n",
    "    name= barriers['name'][i]\n",
    "    barrier_name.append(name)\n",
    "    barrier= barriers['geometry'][i]\n",
    "    barrier= gpd.GeoSeries(barrier)\n",
    "    distance=[]\n",
    "    for j in range(0,len(stations)):\n",
    "        station= stations['geometry'][j]\n",
    "        station= gpd.GeoSeries(station)\n",
    "        dist = barrier.distance(station)\n",
    "        dist = dist.iloc[0]\n",
    "        distance.append(dist)\n",
    "        \n",
    "    min_dist= min(distance)\n",
    "    pos=[e for e, f in enumerate(distance) if f == min_dist]\n",
    "    distance_km=min_dist/1000\n",
    "    min_distance.append(distance_km)\n",
    "    station_nu=stations.Station[pos]\n",
    "    station_nu=station_nu.iloc[0]\n",
    "    station_number.append(station_nu)\n",
    "    \n",
    "df = pd.DataFrame(list(zip(barrier_name, station_number, min_distance)),\n",
    "               columns =['name', 'closest_station','distance_km'])\n",
    "df2  = barriers.merge(df, on='name', how='left')\n",
    "gdf = gpd.GeoDataFrame(df2)\n",
    "gdf.to_file(\"./data/Exceedance/Downeast_Stations.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9487bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate exceedance probability curves \n",
    "\n",
    "# Create folder if it does no exist\n",
    "outdir= './data/Exceedance/PDF'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# Create folder if it does no exist\n",
    "outdir= './data/Exceedance/Probability'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "# Create folder if it does no exist\n",
    "outdir= './data/Exceedance/Curves'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# Load required data (.csv file available in GitHub, see \"Data\" folder)\n",
    "param = pd.read_csv(\"./data/tables/Parameters.csv\", sep=\",\", header=0) # table with parameters from NOAA's report \n",
    "stations = pd.read_csv(\"./data/tables/Stations.csv\", sep=\",\", header=0) # table with station information, including MLR trends (used to detrend historical data)\n",
    "mhhw = pd.read_csv(\"./data/tables/MHHW.csv\", sep=\",\", header=0) # table with MHHW only for stations linked to the barriers islands (info extracted from https://tidesandcurrents.noaa.gov/est/)\n",
    "barriers = gpd.read_file(\"./data/Exceedance/Downeast_Stations.shp\") # shp with Gulf and Atlantic US barriers. Code of closest station and distance to it included as attributes\n",
    "\n",
    "\n",
    "# Loop within US barriers shp to calculate exceedance for each barrier, using parameters from the closest station\n",
    "for i in range(0, len(barriers)):\n",
    "    barrier= barriers['name'][i]\n",
    "    station= barriers['closest_st'][i]\n",
    "    \n",
    "    for j in range(0, len(param)):\n",
    "        if param.Station_Number[j]==station:\n",
    "            c=float(param.Shape_meters[j]) # shape parameter\n",
    "            loc=float(param.Location_meters[j]) # location parameter\n",
    "            scale=float(param.Scale_meters[j]) # scale parameter\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    for k in range(0,len(stations)):\n",
    "        if stations.Station[k]==station:\n",
    "            MSL_trend=float(stations.MSL_Trend[k]) # retrieve MSL trend of that station (in mm)\n",
    "            station_name=stations.Station_Name[k]  \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    for l in range(0,len(mhhw)):\n",
    "        if mhhw.Station[l]==station:\n",
    "            MHHW=mhhw.MHHW[l] # retrieve the local MHHW (in m)\n",
    "            print(MHHW)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # plot probability distribution function\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    x = np.linspace(genextreme.ppf(0.001, c, loc, scale), genextreme.ppf(0.999, c, loc, scale), 100)\n",
    "    ax.plot(x, genextreme.pdf(x, c, loc, scale), 'r-', lw=5, alpha=0.6, label='{0} genextreme pdf'.format(barrier, station_name))\n",
    "    rv = genextreme(c, loc, scale)\n",
    "    ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
    "    \n",
    "    vals = genextreme.ppf([0.001, 0.5, 0.999], c, loc, scale)\n",
    "    np.allclose([0.001, 0.5, 0.999], genextreme.cdf(vals, c, loc, scale))\n",
    "\n",
    "    r = genextreme.rvs(c, loc, scale, size=1000) # sample from the distribution to get water hts > MHHW\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "    plt.title(label='{0}_{1}'.format(barrier, station_name))\n",
    "    plt.show() \n",
    "    fig.savefig('./data/Exceedance/PDF/{0}.png'.format(barrier), dpi=500, facecolor='w')\n",
    "    plt.close(\"all\")\n",
    "       \n",
    "    # calculate exceedance probabilities and return periods\n",
    "    n = len(r) # total observations\n",
    "    df = pd.DataFrame() # initialise dataframe\n",
    "    samples_sorted = list(sorted(r)) # sort 'r' observations ascending\n",
    "    rank = list(range(1, 1 + n)) # rank from 1:n, smallest first\n",
    "    df['Rank'] = rank # make 'Rank' a column of the dataframe (for easier plotting later)\n",
    "    prob = ((n - df['Rank'] + 1) / (n + 1)) # calculate probability\n",
    "    return_years = (1 / prob) # calculate return period (in years) \n",
    "    trend = [x*(MSL_trend/1000) for x in return_years] # calculate linear background trend in MHW (convert MSL_trend to meters)\n",
    "    MaxWL = [x + y for x, y in zip(trend, samples_sorted)] # add trend to >MHHW samples from GEV\n",
    "    MaxWL = [x + MHHW for x in MaxWL] # for real total water level, add background MHHW level\n",
    "\n",
    "    # fill out remaining columns of dataframe (for easier plotting)\n",
    "    df['MaxWL'] = MaxWL\n",
    "    df['Probability'] = prob\n",
    "    df['Return_Pd'] = return_years\n",
    "    df.to_csv(\"./data/Exceedance/Probability/{0}_Exceedance.csv\".format(barrier)) # save data in csv\n",
    "    \n",
    "    # plot exceedance probability curves\n",
    "    sns.set_theme()\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "    \n",
    "    ax.set(xscale=\"log\")\n",
    "    ax.tick_params(left=True, bottom=True)\n",
    "    ax = sns.scatterplot(x=\"Return_Pd\", y=\"MaxWL\", data=df, color= 'r', linewidth=0, s= 25, label='exceedance probability')\n",
    "    ax.set(xlim = (0,150)) # set x axis limits\n",
    "    a =list(df.loc[df['Rank'] == 991, 'MaxWL'])[0] # to find upper limit of y axis\n",
    "    ax.set(ylim = (0, a+0.5)) # set y axis limits\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "    plt.title(label='{0}_{1}'.format(barrier, station_name))\n",
    "    plt.savefig(\"./data/Exceedance/Curves/{0}.png\".format(barrier), dpi=500, facecolor='w')\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0818c070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
