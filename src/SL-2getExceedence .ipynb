{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd42a9b",
   "metadata": {},
   "source": [
    "##  Calculate exceedance probabilities of extreme water levels for NC barrier islands -\n",
    "\n",
    "The purpose of this notebook is to calculate exceedance probabilities of extreme water levels for Downeast using a GEV approach. The parameters (scale, shape, and location) for the closest NOAA CO-OPS (Center for Operational Oceanographic Products and Services) station to each barrier island were extracted from the NOAA Technical Report NOS CO-OPS 067 (Zervas, 2013) available in https://tidesandcurrents.noaa.gov/publications/NOAA_Technical_Report_NOS_COOPS_067a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0527f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import genextreme\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdbaaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set working directory\n",
    "\n",
    "path='..' # introduce path to your working directory\n",
    "os.chdir(path)\n",
    "\n",
    "### Merge all barrier polygons in one single shp \n",
    "\n",
    "# Create folder if it does no exist\n",
    "outdir= './data/Exceedance'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "# Read all shp and merge them in one single dataset\n",
    "folder = Path(\"./data/Davis\")\n",
    "shapefiles = folder.glob(\"*.shp\")\n",
    "gdf = pd.concat([\n",
    "    gpd.read_file(shp)\n",
    "    for shp in shapefiles\n",
    "]).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# For some barrier islands, the column \"name\" has a different format. Convert name to same format so they can be matched later\n",
    "# gdf.loc[gdf.name == 'NC1   -           Core        Banks', 'name'] = 'NC1'\n",
    "# gdf.loc[gdf.name == 'NC2-        Cape     Lookout, NC', 'name'] = 'NC2'\n",
    "# gdf.loc[gdf.name == 'NC3  ShacklefordBanks,     NC', 'name'] = 'NC3'\n",
    "# gdf.loc[gdf.name == 'NC4-        Bogue    Banks,   NC', 'name'] = 'NC4'\n",
    "# gdf.loc[gdf.name == 'NC12Ocracoke  Island,   NC', 'name'] = 'NC12'\n",
    "\n",
    "\n",
    "gdf.to_file('./data/Exceedance/Davis.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6580325e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((-76.44757 34.81854, -76.44111 34.815...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           geometry\n",
       "0  None  POLYGON ((-76.44757 34.81854, -76.44111 34.815..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barriers= gpd.read_file('./data/Exceedance/Davis.shp')\n",
    "barriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70013c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebgoldstein/anaconda3/envs/roads/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:122: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n",
      "/tmp/ipykernel_292618/3457588174.py:7: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  gdf.to_file(\"./data/Exceedance/Stations.shp\")\n"
     ]
    }
   ],
   "source": [
    "### Read stations.csv and convert to geodataframe \n",
    "\n",
    "df = pd.read_csv(\"./data/tables/Stations.csv\", sep=\",\", header=0) \n",
    "df[\"geometry\"] = df[[\"Longitude\", \"Latitude\"]].apply(Point, axis=1)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "gdf.to_file(\"./data/Exceedance/Stations.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b971ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                           geometry\n",
      "0  None  POLYGON ((1675830.089 -352746.971, 1676439.038...\n",
      "     id  closest_station  distance_km\n",
      "0  None          8656483    16.275413\n",
      "     id                                           geometry\n",
      "0  None  POLYGON ((1675830.089 -352746.971, 1676439.038...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292618/2769002449.py:42: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  gdf.to_file(\"./data/Exceedance/Davis_Stations.shp\")\n"
     ]
    }
   ],
   "source": [
    "### Find nearest station to each barrier and its corresponding distance \n",
    "\n",
    "barriers= gpd.read_file('./data/Exceedance/Davis.shp')\n",
    "stations= gpd.read_file('./data/Exceedance/Stations.shp')\n",
    "\n",
    "barriers= barriers.to_crs('esri:102009')\n",
    "stations= stations.to_crs('esri:102009')\n",
    "\n",
    "print(barriers)\n",
    "\n",
    "barrier_id=[]\n",
    "min_distance=[]\n",
    "station_number=[]\n",
    "\n",
    "for i in range(0,len(barriers)):\n",
    "    name= barriers['id'][i]\n",
    "    barrier_id.append(name)\n",
    "    barrier= barriers['geometry'][i]\n",
    "    barrier= gpd.GeoSeries(barrier)\n",
    "    distance=[]\n",
    "    for j in range(0,len(stations)):\n",
    "        station= stations['geometry'][j]\n",
    "        station= gpd.GeoSeries(station)\n",
    "        dist = barrier.distance(station)\n",
    "        dist = dist.iloc[0]\n",
    "        distance.append(dist)\n",
    "        \n",
    "    min_dist= min(distance)\n",
    "    pos=[e for e, f in enumerate(distance) if f == min_dist]\n",
    "    distance_km=min_dist/1000\n",
    "    min_distance.append(distance_km)\n",
    "    station_nu=stations.Station[pos]\n",
    "    station_nu=station_nu.iloc[0]\n",
    "    station_number.append(station_nu)\n",
    "    \n",
    "df = pd.DataFrame(list(zip(barrier_name, station_number, min_distance)),\n",
    "               columns =['id', 'closest_station','distance_km'])\n",
    "print(df)\n",
    "print(barriers)\n",
    "df2  = barriers.merge(df, on='id', how='left')\n",
    "gdf = gpd.GeoDataFrame(df2)\n",
    "gdf.to_file(\"./data/Exceedance/Davis_Stations.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9487bd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "./data/Exceedance/Downeast_Stations.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/_shim.pyx:83\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ./data/Exceedance/Downeast_Stations.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m stations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/tables/Stations.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# table with station information, including MLR trends (used to detrend historical data)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m mhhw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/tables/MHHW.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# table with MHHW only for stations linked to the barriers islands (info extracted from https://tidesandcurrents.noaa.gov/est/)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m barriers \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/Exceedance/Downeast_Stations.shp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# shp with Gulf and Atlantic US barriers. Code of closest station and distance to it included as attributes\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Loop within US barriers shp to calculate exceedance for each barrier, using parameters from the closest station\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(barriers)):\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/geopandas/io/file.py:253\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_file_pyogrio(\n\u001b[1;32m    258\u001b[0m         path_or_bytes, bbox\u001b[38;5;241m=\u001b[39mbbox, mask\u001b[38;5;241m=\u001b[39mmask, rows\u001b[38;5;241m=\u001b[39mrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    259\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/geopandas/io/file.py:294\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[1;32m    295\u001b[0m \n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m# In a future Fiona release the crs attribute of features will\u001b[39;00m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# no longer be a dict, but will behave like a dict. So this should\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;66;03m# be forwards compatible\u001b[39;00m\n\u001b[1;32m    299\u001b[0m         crs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    300\u001b[0m             features\u001b[38;5;241m.\u001b[39mcrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    301\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[1;32m    303\u001b[0m         )\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;66;03m# handle loading the bounding box\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/fiona/env.py:408\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local\u001b[38;5;241m.\u001b[39m_env:\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/fiona/__init__.py:264\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     path \u001b[38;5;241m=\u001b[39m parse_path(fp)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 264\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m schema:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;66;03m# Make an ordered dict of schema properties.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/roads/lib/python3.10/site-packages/fiona/collection.py:162\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[0;32mfiona/ogrext.pyx:540\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_shim.pyx:90\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: ./data/Exceedance/Downeast_Stations.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "### Calculate exceedance probability curves \n",
    "\n",
    "# Create folder if it does no exist\n",
    "outdir= './data/Exceedance/PDF'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# Create folder if it does no exist\n",
    "outdir= './data/Exceedance/Probability'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "# Create folder if it does no exist\n",
    "outdir= './data/Exceedance/Curves'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# Load required data (.csv file available in GitHub, see \"Data\" folder)\n",
    "param = pd.read_csv(\"./data/tables/Parameters.csv\", sep=\",\", header=0) # table with parameters from NOAA's report \n",
    "stations = pd.read_csv(\"./data/tables/Stations.csv\", sep=\",\", header=0) # table with station information, including MLR trends (used to detrend historical data)\n",
    "mhhw = pd.read_csv(\"./data/tables/MHHW.csv\", sep=\",\", header=0) # table with MHHW only for stations linked to the barriers islands (info extracted from https://tidesandcurrents.noaa.gov/est/)\n",
    "barriers = gpd.read_file(\"./data/Exceedance/Downeast_Stations.shp\") # shp with Gulf and Atlantic US barriers. Code of closest station and distance to it included as attributes\n",
    "\n",
    "\n",
    "# Loop within US barriers shp to calculate exceedance for each barrier, using parameters from the closest station\n",
    "for i in range(0, len(barriers)):\n",
    "    barrier= barriers['name'][i]\n",
    "    station= barriers['closest_st'][i]\n",
    "    \n",
    "    for j in range(0, len(param)):\n",
    "        if param.Station_Number[j]==station:\n",
    "            c=float(param.Shape_meters[j]) # shape parameter\n",
    "            loc=float(param.Location_meters[j]) # location parameter\n",
    "            scale=float(param.Scale_meters[j]) # scale parameter\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    for k in range(0,len(stations)):\n",
    "        if stations.Station[k]==station:\n",
    "            MSL_trend=float(stations.MSL_Trend[k]) # retrieve MSL trend of that station (in mm)\n",
    "            station_name=stations.Station_Name[k]  \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    for l in range(0,len(mhhw)):\n",
    "        if mhhw.Station[l]==station:\n",
    "            MHHW=mhhw.MHHW[l] # retrieve the local MHHW (in m)\n",
    "            print(MHHW)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # plot probability distribution function\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    x = np.linspace(genextreme.ppf(0.001, c, loc, scale), genextreme.ppf(0.999, c, loc, scale), 100)\n",
    "    ax.plot(x, genextreme.pdf(x, c, loc, scale), 'r-', lw=5, alpha=0.6, label='{0} genextreme pdf'.format(barrier, station_name))\n",
    "    rv = genextreme(c, loc, scale)\n",
    "    ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
    "    \n",
    "    vals = genextreme.ppf([0.001, 0.5, 0.999], c, loc, scale)\n",
    "    np.allclose([0.001, 0.5, 0.999], genextreme.cdf(vals, c, loc, scale))\n",
    "\n",
    "    r = genextreme.rvs(c, loc, scale, size=1000) # sample from the distribution to get water hts > MHHW\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "    plt.title(label='{0}_{1}'.format(barrier, station_name))\n",
    "    plt.show() \n",
    "    fig.savefig('./data/Exceedance/PDF/{0}.png'.format(barrier), dpi=500, facecolor='w')\n",
    "    plt.close(\"all\")\n",
    "       \n",
    "    # calculate exceedance probabilities and return periods\n",
    "    n = len(r) # total observations\n",
    "    df = pd.DataFrame() # initialise dataframe\n",
    "    samples_sorted = list(sorted(r)) # sort 'r' observations ascending\n",
    "    rank = list(range(1, 1 + n)) # rank from 1:n, smallest first\n",
    "    df['Rank'] = rank # make 'Rank' a column of the dataframe (for easier plotting later)\n",
    "    prob = ((n - df['Rank'] + 1) / (n + 1)) # calculate probability\n",
    "    return_years = (1 / prob) # calculate return period (in years) \n",
    "    trend = [x*(MSL_trend/1000) for x in return_years] # calculate linear background trend in MHW (convert MSL_trend to meters)\n",
    "    MaxWL = [x + y for x, y in zip(trend, samples_sorted)] # add trend to >MHHW samples from GEV\n",
    "    MaxWL = [x + MHHW for x in MaxWL] # for real total water level, add background MHHW level\n",
    "\n",
    "    # fill out remaining columns of dataframe (for easier plotting)\n",
    "    df['MaxWL'] = MaxWL\n",
    "    df['Probability'] = prob\n",
    "    df['Return_Pd'] = return_years\n",
    "    df.to_csv(\"./data/Exceedance/Probability/{0}_Exceedance.csv\".format(barrier)) # save data in csv\n",
    "    \n",
    "    # plot exceedance probability curves\n",
    "    sns.set_theme()\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "    \n",
    "    ax.set(xscale=\"log\")\n",
    "    ax.tick_params(left=True, bottom=True)\n",
    "    ax = sns.scatterplot(x=\"Return_Pd\", y=\"MaxWL\", data=df, color= 'r', linewidth=0, s= 25, label='exceedance probability')\n",
    "    ax.set(xlim = (0,150)) # set x axis limits\n",
    "    a =list(df.loc[df['Rank'] == 991, 'MaxWL'])[0] # to find upper limit of y axis\n",
    "    ax.set(ylim = (0, a+0.5)) # set y axis limits\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "    plt.title(label='{0}_{1}'.format(barrier, station_name))\n",
    "    plt.savefig(\"./data/Exceedance/Curves/{0}.png\".format(barrier), dpi=500, facecolor='w')\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0818c070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
